{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OR9-pMzi7CbD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from PIL import Image\n",
        "import keras\n",
        "from keras.models import Model\n",
        "from keras.layers import Conv2D, MaxPooling2D, Input, Dense, Flatten, Reshape, LeakyReLU, Dropout, Embedding, Conv2DTranspose, Concatenate, BatchNormalization, UpSampling2D,MaxPool2D\n",
        "from keras.layers import  Dropout, Activation\n",
        "from keras.optimizers import Adam, SGD\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
        "from keras import backend as K\n",
        "from tensorflow.keras.utils import plot_model\n",
        "import tensorflow as tf\n",
        "import glob\n",
        "import random\n",
        "import cv2\n",
        "from random import shuffle\n",
        "from numpy import expand_dims\n",
        "from numpy import zeros\n",
        "from numpy import ones\n",
        "from numpy import asarray\n",
        "from numpy.random import randn\n",
        "from numpy.random import randint\n",
        "from keras.datasets.mnist import load_data\n",
        "from keras.optimizers import Adam\n",
        "from numpy import random\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Lambda\n",
        "from keras.layers import Activation\n",
        "from matplotlib import pyplot\n",
        "from keras import backend\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "from keras import regularizers \n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jlTXlKOb7DoC"
      },
      "outputs": [],
      "source": [
        "!wget https://md-datasets-cache-zipfiles-prod.s3.eu-west-1.amazonaws.com/rscbjbr9sj-3.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tP5IypJN7Gxh"
      },
      "outputs": [],
      "source": [
        "!unzip rscbjbr9sj-3.zip "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-P_gYq257HJZ"
      },
      "outputs": [],
      "source": [
        "!unzip ZhangLabData.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sPB5ibWJ7HMS"
      },
      "outputs": [],
      "source": [
        "def load_files():\n",
        "  base = 'CellData/OCT/train/'\n",
        "  folders = [('NORMAL',0),('CNV',1),('DME',2),('DRUSEN',3)]  \n",
        "  \n",
        "  files = []\n",
        "  for folder in folders:\n",
        "    files_folder = os.listdir(f'{base}{folder[0]}')\n",
        "    if folder[0] == 'NORMAL':\n",
        "      files_folder = files_folder[0:26565]\n",
        "    for i in range(len(files_folder)):\n",
        "      files.append((f'{base}{folder[0]}/{files_folder[i]}',folder[1]))\n",
        "  random.seed(4)\n",
        "  random.shuffle(files)\n",
        "  return files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_eyn5AFFdhO"
      },
      "outputs": [],
      "source": [
        "def load_images(files,parts=100,part=1):\n",
        "  files_len = len(files)\n",
        "  files_part = files[int((part-1)*(files_len/parts)):int((part)*(files_len/parts))]\n",
        "  size=(224,224)\n",
        "  images = []\n",
        "  labales = []\n",
        "\n",
        "  for file in files_part:\n",
        "    raw = Image.open(file[0])\n",
        "    raw = raw.resize(size)\n",
        "    raw = np.array(raw)\n",
        "    if(raw.shape == size):\n",
        "      images.append(raw)\n",
        "      labales.append(file[1])\n",
        "      \n",
        "  return np.array(images), np.array(labales)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tkppc7PDV-OJ"
      },
      "outputs": [],
      "source": [
        "def load_images_percent(files,percent=1):\n",
        "  files_len = len(files)\n",
        "  percent_amount = int((files_len * percent)/100)\n",
        "  point_split = int((files_len * 40)/100)\n",
        "  files_part = files[point_split:point_split+percent_amount]\n",
        "  size=(224,224)\n",
        "  images = []\n",
        "  labales = []\n",
        "\n",
        "  for file in files_part:\n",
        "    raw = Image.open(file[0])\n",
        "    raw = raw.resize(size)\n",
        "    raw = np.array(raw)\n",
        "    if(raw.shape == size):\n",
        "      images.append(raw)\n",
        "      labales.append(file[1])\n",
        "      \n",
        "  return np.array(images), np.array(labales)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jH-VCGYKnvt0"
      },
      "outputs": [],
      "source": [
        "def load_images_RAM(datax,datay,parts=100,part=1):\n",
        "  files_len = len(datay)\n",
        "  x_part = datax[int((part-1)*(files_len/parts)):int((part)*(files_len/parts))]\n",
        "  y_part = datay[int((part-1)*(files_len/parts)):int((part)*(files_len/parts))]\n",
        "  return x_part, y_part"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kedELF4x7HOt"
      },
      "outputs": [],
      "source": [
        "files = load_files()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NRh2K-Zkh-L1"
      },
      "outputs": [],
      "source": [
        "datax, datay = load_images(files,1,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "neSO4rPXo788"
      },
      "outputs": [],
      "source": [
        "datax = datax[837:]\n",
        "datay = datay[837:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zL0z4cYf7HRP"
      },
      "outputs": [],
      "source": [
        "train_x, train_y = load_images(files,100,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V7GKJzJvGs0U"
      },
      "outputs": [],
      "source": [
        "# define the standalone discriminator model\n",
        "def define_discriminator(in_shape=(224,224,1), n_classes=4):\n",
        "    # label input\n",
        "    in_label = Input(shape=(1,))\n",
        "    # embedding for categorical input\n",
        "    li = Embedding(n_classes, 50)(in_label)\n",
        "    # scale up to image dimensions with linear activation\n",
        "    n_nodes = in_shape[0] * in_shape[1]\n",
        "    li = Dense(n_nodes)(li)\n",
        "    # reshape to additional channel\n",
        "    li = Reshape((in_shape[0], in_shape[1], 1))(li)\n",
        "    # image input\n",
        "    in_image = Input(shape=in_shape)\n",
        "    # concat label as a channel\n",
        "    merge = Concatenate()([in_image, li])\n",
        "  \n",
        "\n",
        "    ###################################################################\n",
        "    # downsample to 112*112\n",
        "    fe = Conv2D(32, (3,3), strides=(2,2), padding='same')(merge)\n",
        "    fe = LeakyReLU(alpha=0.2)(fe)\n",
        "    # downsample to 56*56\n",
        "    fe = Conv2D(64, (3,3), strides=(2,2), padding='same')(fe)\n",
        "    fe = LeakyReLU(alpha=0.2)(fe)\n",
        "    # downsample to 28*28\n",
        "    fe = Conv2D(64, (3,3), strides=(2,2), padding='same')(fe)\n",
        "    fe = LeakyReLU(alpha=0.2)(fe)\n",
        "    # downsample to 14*14\n",
        "    fe = Conv2D(128, (3,3), strides=(2,2), padding='same')(fe)\n",
        "    fe = LeakyReLU(alpha=0.2)(fe)\n",
        "    # downsample to 7*7\n",
        "    fe = Conv2D(256, (3,3), strides=(2,2), padding='same')(fe)\n",
        "    fe = LeakyReLU(alpha=0.2)(fe)\n",
        "    ###################################################################\n",
        "\n",
        "\n",
        "    # flatten feature maps\n",
        "    fe = Flatten()(fe)\n",
        "    # dropout\n",
        "    fe = Dropout(0.4)(fe)\n",
        "    # output\n",
        "    out_layer = Dense(1, activation='sigmoid')(fe)\n",
        "    # define model\n",
        "    model = Model([in_image, in_label], out_layer)\n",
        "    # compile model\n",
        "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
        "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fzIAVSy4Hg7v"
      },
      "outputs": [],
      "source": [
        "# define the standalone generator model\n",
        "def define_generator(latent_dim, n_classes=4):\n",
        "    # label input\n",
        "    in_label = Input(shape=(1,))\n",
        "    # embedding for categorical input\n",
        "    li = Embedding(n_classes, 50)(in_label)\n",
        "    # linear multiplication\n",
        "    n_nodes = 7 * 7\n",
        "    li = Dense(n_nodes)(li)\n",
        "    # reshape to additional channel\n",
        "    li = Reshape((7, 7, 1))(li)\n",
        "    # image generator input\n",
        "    in_lat = Input(shape=(latent_dim,))\n",
        "    # foundation for 7x7 image\n",
        "    n_nodes = 128 * 7 * 7\n",
        "    gen = Dense(n_nodes)(in_lat)\n",
        "    gen = LeakyReLU(alpha=0.2)(gen)\n",
        "    gen = Reshape((7, 7, 128))(gen)\n",
        "    # merge image gen and label input\n",
        "    merge = Concatenate()([gen, li])\n",
        "    #########################################################\n",
        "    # upsample to 14x14\n",
        "    gen = Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')(merge)\n",
        "    gen = LeakyReLU(alpha=0.2)(gen)\n",
        "    # upsample to 28x28\n",
        "    gen = Conv2DTranspose(64, (4,4), strides=(2,2), padding='same')(gen)\n",
        "    gen = LeakyReLU(alpha=0.2)(gen)\n",
        "    # upsample to 56x56\n",
        "    gen = Conv2DTranspose(64, (4,4), strides=(2,2), padding='same')(gen)\n",
        "    gen = LeakyReLU(alpha=0.2)(gen)\n",
        "    # upsample to 112x112\n",
        "    gen = Conv2DTranspose(32, (4,4), strides=(2,2), padding='same')(gen)\n",
        "    gen = LeakyReLU(alpha=0.2)(gen)\n",
        "    # upsample to 224x224\n",
        "    gen = Conv2DTranspose(16, (4,4), strides=(2,2), padding='same')(gen)\n",
        "    gen = LeakyReLU(alpha=0.2)(gen)\n",
        "    #########################################################\n",
        "    # output\n",
        "    out_layer = Conv2D(1, (7,7), activation='tanh', padding='same')(gen)\n",
        "    # define model\n",
        "    model = Model([in_lat, in_label], out_layer)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5SoV3JiZHs-7"
      },
      "outputs": [],
      "source": [
        "# define the combined generator and discriminator model, for updating the generator\n",
        "def define_gan(g_model, d_model):\n",
        "    # make weights in the discriminator not trainable\n",
        "    d_model.trainable = False\n",
        "    # get noise and label inputs from generator model\n",
        "    gen_noise, gen_label = g_model.input\n",
        "    # get image output from the generator model\n",
        "    gen_output = g_model.output\n",
        "    # connect image output and label input from generator as inputs to discriminator\n",
        "    gan_output = d_model([gen_output, gen_label])\n",
        "    # define gan model as taking noise and label and outputting a classification\n",
        "    model = Model([gen_noise, gen_label], gan_output)\n",
        "    # compile model\n",
        "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
        "    model.compile(loss='binary_crossentropy', optimizer=opt)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kTMQU7VPH8B3"
      },
      "outputs": [],
      "source": [
        "####Helper Functions####\n",
        "\n",
        "\n",
        "def show_images2(gen):\n",
        "    z_input, labels_input = generate_latent_points(100, 16)\n",
        "    gen_imgs = gen.predict([z_input, labels_input])\n",
        "\n",
        "    # gen_imgs = gen.predict(z)\n",
        "    gen_imgs = 0.5*gen_imgs + 0.5\n",
        "\n",
        "    fig,axs = plt.subplots(4,4,figsize=(4,4),sharey=True,sharex=True)\n",
        "\n",
        "    cnt=0\n",
        "    for i in range(4):\n",
        "        for j in range(4):\n",
        "            axs[i, j].imshow(gen_imgs[cnt,:,:,0],cmap='gray')\n",
        "            axs[i, j].axis('off')\n",
        "            cnt+=1\n",
        "\n",
        "    fig.show()\n",
        "    plt.show()\n",
        "\n",
        "# load fashion mnist images\n",
        "def load_real_samples(i,l):\n",
        "    # load dataset\n",
        "    # (trainX, trainy), (_, _) = load_data()\n",
        "    trainX = i\n",
        "    trainy = l\n",
        "    # expand to 3d, e.g. add channels\n",
        "    X = expand_dims(trainX, axis=-1)\n",
        "    # convert from ints to floats\n",
        "    X = X.astype('float32')\n",
        "    # scale from [0,255] to [-1,1]\n",
        "    X = (X - 127.5) / 127.5\n",
        "    print(X.shape, trainy.shape)\n",
        "    return [X, trainy]\n",
        "\n",
        "# # select real samples\n",
        "def generate_real_samples(dataset, n_samples):\n",
        "    # split into images and labels\n",
        "    images, labels = dataset\n",
        "    # choose random instances\n",
        "    ix = randint(0, images.shape[0], n_samples)\n",
        "    # select images and labels\n",
        "    X, labels = images[ix], labels[ix]\n",
        "    # generate class labels\n",
        "    y = ones((n_samples, 1))\n",
        "    return [X, labels], y\n",
        "\n",
        "# generate points in latent space as input for the generator\n",
        "def generate_latent_points(latent_dim, n_samples, n_classes=4):\n",
        "    # generate points in the latent space\n",
        "    x_input = randn(latent_dim * n_samples)\n",
        "    # reshape into a batch of inputs for the network\n",
        "    z_input = x_input.reshape(n_samples, latent_dim)\n",
        "    # generate labels\n",
        "    labels = randint(0, n_classes, n_samples)\n",
        "    return [z_input, labels]\n",
        "\n",
        "# use the generator to generate n fake examples, with class labels\n",
        "def generate_fake_samples(generator, latent_dim, n_samples):\n",
        "    # generate points in latent space\n",
        "    z_input, labels_input = generate_latent_points(latent_dim, n_samples)\n",
        "    # predict outputs\n",
        "    images = generator.predict([z_input, labels_input])\n",
        "    # create class labels\n",
        "    y = zeros((n_samples, 1))\n",
        "    return [images, labels_input], y\n",
        "\n",
        "def summarize_performance(step, g_model, c_model, latent_dim, dataset, n_samples=100):\n",
        "    X, y = dataset\n",
        "    _, acc = c_model.evaluate(X, y, verbose=0)\n",
        "    print('Classifier preci: %.3f%%' % (acc * 100))\n",
        "    show_images2(g_model)\n",
        "\n",
        "def load_real_samples_test(a,b):\n",
        "    # load dataset\n",
        "    # (trainX, trainy), (_, _) = load_data()\n",
        "    trainX = a\n",
        "    trainy = b\n",
        "    # expand to 3d, e.g. add channels\n",
        "    X = expand_dims(trainX, axis=-1)\n",
        "    # convert from ints to floats\n",
        "    X = X.astype('float32')\n",
        "    # scale from [0,255] to [-1,1]\n",
        "    X = (X - 127.5) / 127.5\n",
        "    print(X.shape, trainy.shape)\n",
        "    return [X, trainy]\n",
        "\n",
        "\n",
        "def calc_metrics(y_true, y_predi, Nclass=4 ):\n",
        "  Precision = []\n",
        "  F1 =  []\n",
        "  Specifity =   []\n",
        "  sensitivity =   []\n",
        "  Accs=   []\n",
        "  for c in range(Nclass):\n",
        "      TP = np.sum( (y_true == c)&(y_predi==c) )\n",
        "      FP = np.sum( (y_true != c)&(y_predi==c) )\n",
        "      FN = np.sum( (y_true == c)&(y_predi != c)) \n",
        "      TN = np.sum( (y_true != c)&(y_predi != c)) \n",
        "\n",
        "      prec = TP/float(TP+FP)\n",
        "      Precision.append(prec)\n",
        "      f1= (2*TP)/float((2*TP) + FP + FN)\n",
        "      F1.append(f1)\n",
        "      ACC=(TP+TN)/float(TP+FN + TN + FP)\n",
        "      Accs.append(ACC)\n",
        "      SPE=TN/float(TN+FP)\n",
        "      Specifity.append(SPE)\n",
        "      SEN=TP/float(TP+FN)\n",
        "      sensitivity.append(SEN)\n",
        "  return Precision, sensitivity, Specifity, F1, Accs\n",
        "\n",
        "\n",
        "def result_calc_RAM(classifier,amount=100):\n",
        "  split_point = int(((100-amount) * len(datay)) / 100)\n",
        "  prec_array = []\n",
        "  accu_array = []\n",
        "  sen_array = []\n",
        "  sp_array = []\n",
        "  fscore_array = []\n",
        "  ytrue = []\n",
        "  yped = []\n",
        "  for i in range(100):\n",
        "    print(i)\n",
        "    atest,btest = load_images_RAM(datax[split_point:],datay[split_point:],100,i+1)\n",
        "    x, y = load_real_samples_test(atest,btest)\n",
        "    y = to_categorical(y,num_classes=8)\n",
        "    y_true = np.argmax(y, axis=1)\n",
        "    y_pred = classifier.predict(x).argmax(axis=1)\n",
        "    ytrue.extend(y_true)\n",
        "    yped.extend(y_pred)\n",
        "    preci, sensitivity, sp, fscore, accu=calc_metrics(y_true, y_pred)\n",
        "    prec_array.append(preci)\n",
        "    sen_array.append(sensitivity)\n",
        "    sp_array.append(sp)\n",
        "    fscore_array.append(fscore)\n",
        "    accu_array.append(accu)\n",
        "  prec_array = np.array(prec_array).mean(axis=0)\n",
        "  sen_array = np.array(sen_array).mean(axis=0)\n",
        "  sp_array = np.array(sp_array).mean(axis=0)\n",
        "  fscore_array = np.array(fscore_array).mean(axis=0)\n",
        "  accu_array = np.array(accu_array).mean(axis=0)\n",
        "  return {\n",
        "      \"precision\": [prec_array,prec_array.mean()],\n",
        "      \"se\": [sen_array,sen_array.mean()],\n",
        "      \"sp\": [sp_array,sp_array.mean()],\n",
        "      \"f1\": [fscore_array,fscore_array.mean()],\n",
        "      \"accu\": [accu_array,accu_array.mean()],\n",
        "  }, ytrue, yped\n",
        "\n",
        "def result_calc(classifier):\n",
        "  prec_array = []\n",
        "  accu_array = []\n",
        "  sen_array = []\n",
        "  sp_array = []\n",
        "  fscore_array = []\n",
        "  for i in range(100):\n",
        "    print(i)\n",
        "    atest,btest = load_images(files,100,i+1)\n",
        "    x, y = load_real_samples_test(atest,btest)\n",
        "    y = to_categorical(y,num_classes=8)\n",
        "    y_true = np.argmax(y, axis=1)\n",
        "    y_pred = classifier.predict(x).argmax(axis=1)\n",
        "    preci, sensitivity, sp, fscore, accu=calc_metrics(y_true, y_pred)\n",
        "    prec_array.append(preci)\n",
        "    sen_array.append(sensitivity)\n",
        "    sp_array.append(sp)\n",
        "    fscore_array.append(fscore)\n",
        "    accu_array.append(accu)\n",
        "  prec_array = np.array(prec_array).mean(axis=0)\n",
        "  sen_array = np.array(sen_array).mean(axis=0)\n",
        "  sp_array = np.array(sp_array).mean(axis=0)\n",
        "  fscore_array = np.array(fscore_array).mean(axis=0)\n",
        "  accu_array = np.array(accu_array).mean(axis=0)\n",
        "  return {\n",
        "      \"precision\": [prec_array,prec_array.mean()],\n",
        "      \"se\": [sen_array,sen_array.mean()],\n",
        "      \"sp\": [sp_array,sp_array.mean()],\n",
        "      \"f1\": [fscore_array,fscore_array.mean()],\n",
        "      \"accu\": [accu_array,accu_array.mean()],\n",
        "  }\n",
        "\n",
        "def cus_specificity(y_true, y_pred):\n",
        "    true_negatives = K.sum(K.round(K.clip((1 - y_true) * (1 - y_pred), 0, 1)))\n",
        "    possible_negatives = K.sum(K.round(K.clip(1 - y_true, 0, 1)))\n",
        "    return true_negatives / (possible_negatives + K.epsilon())\n",
        "\n",
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
        "\n",
        "def shuffle_data(a,b):\n",
        "    c = list(zip(a, b))\n",
        "    random.shuffle(c)\n",
        "    a, b = zip(*c)\n",
        "    return np.array(a),np.array(b)\n",
        "\n",
        "def select_imgs(number,numclass,gmodel):\n",
        "  batch_number = int(number/numclass)\n",
        "  result_imgse = []\n",
        "  result_lbs = []\n",
        "  for i in range(numclass):\n",
        "    x_input = randn(100 * batch_number)\n",
        "    # reshape into a batch of inputs for the network\n",
        "    z_input = x_input.reshape(batch_number, 100)\n",
        "    # generate labels\n",
        "    labels = np.array([i]*batch_number)\n",
        "    gen_imgs = gmodel.predict([z_input, labels])\n",
        "    result_imgse.extend(gen_imgs)\n",
        "    result_lbs.extend([i]*batch_number)\n",
        "  return np.array(result_imgse), np.array(result_lbs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_6Qi0TQ7Sty"
      },
      "outputs": [],
      "source": [
        "# train the generator and discriminator\n",
        "def train(g_model, d_model, gan_model, dataset, latent_dim, n_epochs=10000, n_batch=128):\n",
        "    bat_per_epo = int(dataset[0].shape[0] / n_batch)\n",
        "    half_batch = int(n_batch / 2)\n",
        "    # manually enumerate epochs\n",
        "    for i in range(n_epochs):\n",
        "        print(i)\n",
        "        # enumerate batches over the training set\n",
        "        for j in range(bat_per_epo):\n",
        "            # get randomly selected 'real' samples\n",
        "            [X_real, labels_real], y_real = generate_real_samples(dataset, half_batch)\n",
        "            # update discriminator model weights\n",
        "            d_loss1, _ = d_model.train_on_batch([X_real, labels_real], y_real)\n",
        "            # generate 'fake' examples\n",
        "            [X_fake, labels], y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
        "            # update discriminator model weights\n",
        "            d_loss2, _ = d_model.train_on_batch([X_fake, labels], y_fake)\n",
        "            # prepare points in latent space as input for the generator\n",
        "            [z_input, labels_input] = generate_latent_points(latent_dim, n_batch)\n",
        "            # create inverted labels for the fake samples\n",
        "            y_gan = ones((n_batch, 1))\n",
        "            # update the generator via the discriminator's error\n",
        "            g_loss = gan_model.train_on_batch([z_input, labels_input], y_gan)\n",
        "            # summarize loss on this batch\n",
        "            # print('>%d, %d/%d, d1=%.3f, d2=%.3f g=%.3f' %\n",
        "            #   (i+1, j+1, bat_per_epo, d_loss1, d_loss2, g_loss))\n",
        "        if (i+1) % 10 == 0:\n",
        "            print(i)\n",
        "            show_images2(g_model)\n",
        "\n",
        "# size of the latent space\n",
        "latent_dim = 100\n",
        "# create the discriminator\n",
        "d_model = define_discriminator()\n",
        "# create the generator\n",
        "g_model = define_generator(latent_dim)\n",
        "\n",
        "# create the gan\n",
        "gan_model = define_gan(g_model, d_model)\n",
        "# load image data\n",
        "x,y = load_images(files,100,1)\n",
        "dataset = load_real_samples(x,y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x2e1wDHrII-x"
      },
      "outputs": [],
      "source": [
        "train(g_model, d_model, gan_model, dataset, latent_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iYaCxrXk7HiA"
      },
      "outputs": [],
      "source": [
        "def define_classifier(in_shape=(224,224,1), n_classes=8):\n",
        "    # image input\n",
        "    in_image = Input(shape=in_shape)\n",
        "    # downsample\n",
        "    fe = Conv2D(128, (3,3), strides=(1,1), padding='same',activation='relu')(in_image)\n",
        "    fe = MaxPool2D()(fe)\n",
        "    fe = Dropout(0.4)(fe)\n",
        "\n",
        "    # fe = LeakyReLU(alpha=0.2)(fe)\n",
        "\n",
        "    fe = Conv2D(32, (3,3), strides=(2,2), padding='same',activation='relu')(fe)\n",
        "    fe = MaxPool2D()(fe)\n",
        "    fe = Dropout(0.4)(fe)\n",
        "\n",
        "    fe = Conv2D(32, (3,3), strides=(1,1), padding='same',activation='relu')(fe)\n",
        "    fe = MaxPool2D()(fe)\n",
        "    fe = Dropout(0.4)(fe)\n",
        "\n",
        "    # fe = LeakyReLU(alpha=0.2)(fe)\n",
        "    \n",
        "    fe = Conv2D(128, (3,3), strides=(1,1), padding='same',activation='relu')(fe)\n",
        "    fe = MaxPool2D()(fe)\n",
        "    fe = Dropout(0.4)(fe)\n",
        "\n",
        "\n",
        "    fe = Conv2D(32, (3,3), strides=(1,1), padding='same',activation='relu')(fe)\n",
        "    fe = MaxPool2D()(fe)\n",
        "    fe = Dropout(0.4)(fe)\n",
        "\n",
        "\n",
        "    fe = Conv2D(128, (3,3), strides=(2,2), padding='same',activation='relu')(fe)\n",
        "    fe = MaxPool2D()(fe)\n",
        "    fe = Dropout(0.4)(fe)\n",
        "\n",
        "    \n",
        "\n",
        "    # fe = LeakyReLU(alpha=0.2)(fe)\n",
        "\n",
        "\n",
        "\n",
        "    # flatten feature maps\n",
        "    fe = Flatten()(fe)\n",
        "    # dropout\n",
        "    # output layer nodes\n",
        "    # fe = Dense(100)(fe)\n",
        "    \n",
        "    fe = Dropout(0.4)(fe)\n",
        "    \n",
        "    fe = Dense(n_classes)(fe)\n",
        "    # supervised output\n",
        "    c_out_layer = Activation('softmax')(fe)\n",
        "    # define and compile supervised discriminator model\n",
        "    c_model = Model(in_image, c_out_layer)\n",
        "    # c_model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(lr=0.0002, beta_1=0.5), metrics=['accuracy'])\n",
        "    c_model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(),\n",
        "              metrics=['accuracy',tf.keras.metrics.Recall(), cus_specificity, f1_m ])\n",
        "    return  c_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UcUeY46kipDF"
      },
      "outputs": [],
      "source": [
        "def classifier_1(in_shape=(224,224,1), n_classes=8):\n",
        "  model = models.Sequential()\n",
        "  model.add(layers.Conv2D(6, (7, 7), strides=(2,2), padding='same', data_format=\"channels_last\", kernel_initializer='he_normal', bias_initializer='he_normal', activation=None, input_shape=in_shape))\n",
        "  # model.add(BatchNormalization())\n",
        "  model.add(layers.PReLU())\n",
        "  model.add(layers.Conv2D(16,(7,7), strides=(2,2), padding='same', data_format=\"channels_last\", kernel_initializer='he_normal', bias_initializer='he_normal', activation=None))\n",
        "  # model.add(BatchNormalization())\n",
        "  model.add(layers.PReLU())\n",
        "  model.add(layers.MaxPooling2D((2, 2), strides=2))\n",
        "  model.add(layers.Dropout(0.25))\n",
        "\n",
        "  model.add(layers.Conv2D(36,(5,5), strides=(1,1), padding='same', data_format=\"channels_last\", kernel_initializer='he_normal', bias_initializer='he_normal', activation=None))\n",
        "  # model.add(BatchNormalization())\n",
        "  model.add(layers.PReLU())\n",
        "  model.add(layers.Conv2D(36,(5,5), strides=(1,1), padding='same', data_format=\"channels_last\", kernel_initializer='he_normal', bias_initializer='he_normal', activation=None))\n",
        "  # model.add(BatchNormalization())\n",
        "  model.add(layers.PReLU())\n",
        "  model.add(layers.MaxPooling2D((2, 2), strides=2))\n",
        "  model.add(layers.Dropout(0.25))\n",
        "\n",
        "  model.add(layers.Conv2D(64,(3,3), strides=(1,1), padding='same', data_format=\"channels_last\", kernel_initializer='he_normal', bias_initializer='he_normal', activation=None))\n",
        "  # model.add(BatchNormalization())\n",
        "  model.add(layers.PReLU())\n",
        "  model.add(layers.Dropout(0.25))\n",
        "  model.add(layers.Flatten())\n",
        "  model.add(layers.Dense(64, kernel_initializer='he_normal', bias_initializer='he_normal', kernel_regularizer=regularizers.l2(0.01) , activation=None))\n",
        "  model.add(layers.PReLU())\n",
        "  model.add(layers.Dropout(0.25))\n",
        "  model.add(layers.Dense(n_classes, kernel_initializer='he_normal', bias_initializer='he_normal', activation='softmax'))\n",
        "  # model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
        "  model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(),\n",
        "              metrics=['accuracy',tf.keras.metrics.Recall(), cus_specificity, f1_m ])\n",
        "  return  model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32Z8D4505Dd7"
      },
      "outputs": [],
      "source": [
        "def classifier_2(in_shape=(224,224,1), n_classes=8):\n",
        "  model = models.Sequential()\n",
        "  model.add(layers.Conv2D(64, (3, 3), strides=(1,1), padding='same', data_format=\"channels_last\", kernel_initializer='he_normal', bias_initializer='he_normal', activation=None, input_shape=in_shape))\n",
        "  model.add(layers.LeakyReLU(alpha=0.333))\n",
        "  model.add(layers.Conv2D(64, (3, 3), strides=(1,1), padding='same', data_format=\"channels_last\", kernel_initializer='he_normal', bias_initializer='he_normal', activation=None))\n",
        "  model.add(layers.LeakyReLU(alpha=0.333))\n",
        "  model.add(layers.MaxPooling2D((3, 3), strides=2))\n",
        "\n",
        "  model.add(layers.Conv2D(128, (3, 3), strides=(1,1), padding='same', data_format=\"channels_last\", kernel_initializer='he_normal', bias_initializer='he_normal',  activation=None))\n",
        "  model.add(layers.LeakyReLU(alpha=0.333))\n",
        "  model.add(layers.Conv2D(128, (3, 3), strides=(1,1), padding='same', data_format=\"channels_last\", kernel_initializer='he_normal', bias_initializer='he_normal',  activation=None))\n",
        "  model.add(layers.LeakyReLU(alpha=0.333))\n",
        "  model.add(layers.MaxPooling2D((3, 3), strides=2))\n",
        "\n",
        "  model.add(layers.Flatten())\n",
        "  model.add(layers.Dense(256, kernel_initializer='he_normal', bias_initializer='he_normal', kernel_regularizer=regularizers.l2(0.01) , activation=None))\n",
        "  model.add(layers.LeakyReLU(alpha=0.333))\n",
        "  model.add(layers.Dropout(0.5))\n",
        "  model.add(layers.Dense(256, kernel_initializer='he_normal', bias_initializer='he_normal', kernel_regularizer=regularizers.l2(0.01) , activation=None))\n",
        "  model.add(layers.LeakyReLU(alpha=0.333))\n",
        "  model.add(layers.Dropout(0.5))\n",
        "  model.add(layers.Dense(n_classes, kernel_initializer='he_normal', bias_initializer='he_normal', activation='softmax'))\n",
        "  model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(),\n",
        "              metrics=['accuracy',tf.keras.metrics.Recall(), cus_specificity, f1_m ])\n",
        "  return  model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k-pGkbxS8J01"
      },
      "outputs": [],
      "source": [
        "def classifier_3(in_shape=(224,224,1), n_classes=8):\n",
        "  model = models.Sequential()\n",
        "  model.add(layers.Conv2D(64, (3, 3), strides=(1,1), padding='same', data_format=\"channels_last\", kernel_initializer='he_normal', bias_initializer='he_normal', activation=None, input_shape=in_shape))\n",
        "  model.add(layers.LeakyReLU(alpha=0.333))\n",
        "  model.add(layers.Conv2D(64, (3, 3), strides=(1,1), padding='same', data_format=\"channels_last\", kernel_initializer='he_normal', bias_initializer='he_normal', activation=None))\n",
        "  model.add(layers.LeakyReLU(alpha=0.333))\n",
        "  model.add(layers.Conv2D(64, (3, 3), strides=(1,1), padding='same', data_format=\"channels_last\", kernel_initializer='he_normal', bias_initializer='he_normal', activation=None))\n",
        "  model.add(layers.LeakyReLU(alpha=0.333))\n",
        "  model.add(layers.MaxPooling2D((3, 3), strides=2))\n",
        "\n",
        "  model.add(layers.Conv2D(128, (3, 3), strides=(1,1), padding='same', data_format=\"channels_last\", kernel_initializer='he_normal', bias_initializer='he_normal',  activation=None))\n",
        "  model.add(layers.LeakyReLU(alpha=0.333))\n",
        "  model.add(layers.Conv2D(128, (3, 3), strides=(1,1), padding='same', data_format=\"channels_last\", kernel_initializer='he_normal', bias_initializer='he_normal',  activation=None))\n",
        "  model.add(layers.LeakyReLU(alpha=0.333))\n",
        "  model.add(layers.Conv2D(128, (3, 3), strides=(1,1), padding='same', data_format=\"channels_last\", kernel_initializer='he_normal', bias_initializer='he_normal',  activation=None))\n",
        "  model.add(layers.LeakyReLU(alpha=0.333))\n",
        "  model.add(layers.MaxPooling2D((3, 3), strides=2))\n",
        "\n",
        "  model.add(layers.Flatten())\n",
        "  model.add(layers.Dense(256, kernel_initializer='he_normal', bias_initializer='he_normal', kernel_regularizer=regularizers.l2(0.01) , activation=None))\n",
        "  model.add(layers.LeakyReLU(alpha=0.333))\n",
        "  model.add(layers.Dropout(0.1))\n",
        "  model.add(layers.Dense(256, kernel_initializer='he_normal', bias_initializer='he_normal', kernel_regularizer=regularizers.l2(0.01) , activation=None))\n",
        "  model.add(layers.LeakyReLU(alpha=0.333))\n",
        "  model.add(layers.Dropout(0.1))\n",
        "  model.add(layers.Dense(n_classes, kernel_initializer='he_normal', bias_initializer='he_normal', activation='softmax'))\n",
        "  \n",
        "  model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(),\n",
        "              metrics=['accuracy',tf.keras.metrics.Recall(), cus_specificity, f1_m ])\n",
        "  return  model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9nZCLzPq5WaV"
      },
      "outputs": [],
      "source": [
        "def classifier_4(in_shape=(224,224,1), n_classes=8):\n",
        "  model = models.Sequential()\n",
        "  model.add(layers.Conv2D(32, (3, 3), strides=(1,1), padding='same', data_format=\"channels_last\", kernel_initializer='he_normal', bias_initializer='he_normal', activation=None, input_shape=in_shape))\n",
        "  model.add(layers.ReLU())\n",
        "  # model.add(layers.Dropout(0.5))\n",
        "  model.add(layers.Conv2D(64, (3, 3), strides=(1,1), padding='same', data_format=\"channels_last\", kernel_initializer='he_normal', bias_initializer='he_normal', activation=None))\n",
        "  model.add(layers.ReLU())\n",
        "  # model.add(layers.Dropout(0.5))\n",
        "  model.add(layers.MaxPooling2D((2, 2), strides=2))\n",
        "  # model.add(BatchNormalization())\n",
        "  # model.add(layers.Dropout(0.5))\n",
        "\n",
        "  model.add(layers.Conv2D(128, (3, 3), strides=(1,1), padding='same', data_format=\"channels_last\", kernel_initializer='he_normal', bias_initializer='he_normal', activation=None))\n",
        "  model.add(layers.ReLU())\n",
        "  # model.add(layers.Dropout(0.5))\n",
        "  model.add(layers.Conv2D(256, (3, 3), strides=(1,1), padding='same', data_format=\"channels_last\", kernel_initializer='he_normal', bias_initializer='he_normal', activation=None))\n",
        "  model.add(layers.ReLU())\n",
        "  # model.add(layers.Dropout(0.5))\n",
        "  model.add(layers.MaxPooling2D((2, 2), strides=2))\n",
        "  # model.add(BatchNormalization())\n",
        "  # model.add(layers.Dropout(0.5))\n",
        "\n",
        "  model.add(layers.Flatten())\n",
        "  model.add(layers.Dense(256, kernel_initializer='he_normal', bias_initializer='he_normal', kernel_regularizer=regularizers.l2(0.01) , activation=None))\n",
        "  model.add(layers.ReLU())\n",
        "  model.add(layers.Dense(n_classes, kernel_initializer='he_normal', bias_initializer='he_normal', activation='softmax'))\n",
        "  model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(),\n",
        "              metrics=['accuracy',tf.keras.metrics.Recall(), cus_specificity, f1_m ])\n",
        "  return  model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UqHAxQgd8iUN"
      },
      "outputs": [],
      "source": [
        "def classifier_5(in_shape=(224,224,1), n_classes=8):\n",
        "  model = models.Sequential()\n",
        "  model.add(layers.Conv2D(128, (3, 3), strides=(1,1), padding='same', data_format=\"channels_last\", kernel_initializer='he_normal', bias_initializer='he_normal', activation=None, input_shape=in_shape))\n",
        "  model.add(layers.ReLU())\n",
        "  model.add(layers.Conv2D(128, (3, 3), strides=(1,1), padding='same', data_format=\"channels_last\", kernel_initializer='he_normal', bias_initializer='he_normal', activation=None))\n",
        "  model.add(layers.ReLU())\n",
        "  model.add(layers.MaxPooling2D((2, 2), strides=2))\n",
        "\n",
        "  model.add(layers.Conv2D(256, (3, 3), strides=(1,1), padding='same', data_format=\"channels_last\", kernel_initializer='he_normal', bias_initializer='he_normal', activation=None))\n",
        "  model.add(layers.ReLU())\n",
        "  model.add(layers.Conv2D(256, (3, 3), strides=(1,1), padding='same', data_format=\"channels_last\", kernel_initializer='he_normal', bias_initializer='he_normal', activation=None))\n",
        "  model.add(layers.ReLU())\n",
        "\n",
        "  model.add(layers.Flatten())\n",
        "  model.add(layers.Dense(512, kernel_initializer='he_normal', bias_initializer='he_normal', kernel_regularizer=regularizers.l2(0.01) , activation=None))\n",
        "  model.add(layers.ReLU())\n",
        "  model.add(layers.Dropout(0.5))\n",
        "  model.add(layers.Dense(512, kernel_initializer='he_normal', bias_initializer='he_normal', kernel_regularizer=regularizers.l2(0.01) , activation=None))\n",
        "  model.add(layers.ReLU())\n",
        "  model.add(layers.Dropout(0.5))\n",
        "  model.add(layers.Dense(n_classes, kernel_initializer='he_normal', bias_initializer='he_normal', activation='softmax'))\n",
        "  model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(),\n",
        "              metrics=['accuracy',tf.keras.metrics.Recall(), cus_specificity, f1_m ])\n",
        "  return  model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-v9x0M_J7uzC"
      },
      "outputs": [],
      "source": [
        "classifier = define_classifier()\n",
        "# classifier = classifier_1()\n",
        "\n",
        "# print(classifier.summary())\n",
        "\n",
        "# x,y = load_images(files,30,15)\n",
        "imgs,labels = load_images_percent(files,20)\n",
        "imgs, labels = load_real_samples(x,y)\n",
        "\n",
        "labels = to_categorical(labels,num_classes=8)\n",
        "training = classifier.fit(x=imgs,y=labels,batch_size=64,epochs=50,verbose=1)    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ACOR1Zei79Gi"
      },
      "outputs": [],
      "source": [
        "gen_imgs , pseudo_labels = select_imgs(imgs.shape[0],4,g_model)\n",
        "pseudo_labels += 4\n",
        "pseudo_labels = to_categorical(pseudo_labels,num_classes=8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rBINuMTe8GWR"
      },
      "outputs": [],
      "source": [
        "whole_imgs = np.append(gen_imgs, imgs,axis=0)\n",
        "whole_labels = np.append(pseudo_labels, labels,axis=0)\n",
        "whole_imgs,whole_labels =  shuffle_data(list(whole_imgs),list(whole_labels))\n",
        "classifier.fit(x=whole_imgs,y=whole_labels ,batch_size=64,epochs=200,verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZtYet7bJ8Hn5"
      },
      "outputs": [],
      "source": [
        "loss_array = []\n",
        "acc_array = []\n",
        "sen_array = []\n",
        "sp_array = []\n",
        "fscore_array = []\n",
        "for i in range(100):\n",
        "  print(i)\n",
        "  atest,btest = load_images(files,100,i+1)\n",
        "  x, y = load_real_samples_test(atest,btest)\n",
        "  y = to_categorical(y,num_classes=8)\n",
        "  loss, accuracy, sensitivity, sp, fscore = classifier.evaluate(x,y)\n",
        "  loss_array.append(loss)\n",
        "  acc_array.append(accuracy)\n",
        "  sen_array.append(sensitivity)\n",
        "  sp_array.append(sp)\n",
        "  fscore_array.append(fscore)\n",
        "\n",
        "\n",
        "print(\"accuracy\",np.array(acc_array).mean())\n",
        "print(\"se\",np.array(sen_array).mean())\n",
        "print(\"sp\",np.array(sp_array).mean())\n",
        "print(\"f_score\",np.array(fscore_array).mean()) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6iNMlFQlOM9"
      },
      "outputs": [],
      "source": [
        "loss_array = []\n",
        "acc_array = []\n",
        "sen_array = []\n",
        "sp_array = []\n",
        "fscore_array = []\n",
        "for i in range(100):\n",
        "  print(i)\n",
        "  atest,btest = load_images_RAM(datax,datay,100,i+1)\n",
        "  x, y = load_real_samples_test(atest,btest)\n",
        "  y = to_categorical(y,num_classes=8)\n",
        "  loss, accuracy, sensitivity, sp, fscore = classifier.evaluate(x,y)\n",
        "  loss_array.append(loss)\n",
        "  acc_array.append(accuracy)\n",
        "  sen_array.append(sensitivity)\n",
        "  sp_array.append(sp)\n",
        "  fscore_array.append(fscore)\n",
        "\n",
        "\n",
        "print(\"accuracy\",np.array(acc_array).mean())\n",
        "print(\"se\",np.array(sen_array).mean())\n",
        "print(\"sp\",np.array(sp_array).mean())\n",
        "print(\"f_score\",np.array(fscore_array).mean()) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-eAjLZyv6GI"
      },
      "outputs": [],
      "source": [
        "#20_150\n",
        "np.savetxt(\"myc_20.csv\", np.array([yt, yp]), delimiter=\",\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-pkgVsffRMB5"
      },
      "outputs": [],
      "source": [
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "flRrs4N3hMOf"
      },
      "outputs": [],
      "source": [
        "result,yt,yp = result_calc_RAM(classifier,80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wYit9yNzaEPQ"
      },
      "outputs": [],
      "source": [
        "result"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}